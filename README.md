# `cookiecutter-data-engineering` README

This repo contains a template which can be used to build out new Python
packages within CTS.

## Usage

1. Install `cookiecutter` on your system (or use the provided devcontainer):

    ```sh
    pip3 install cookiecutter
    ```

1. download the template you want to use and customise it:

    ```sh
    cookiecutter git@github.com:Cloud-Technology-Solutions/cookiecutter-data-engineering.git
    ```

## Pre-requisites

### Software packages
Please ensure that the following software is installed on your local machine:

- [Kind](https://kind.sigs.k8s.io/)
- [Helm](https://helm.sh/)
- [jq](https://stedolan.github.io/jq/)
- [yq](https://github.com/mikefarah/yq)


## Get started quickly

- Run poetry shell to create a virtual environment
- Run poetry install to install the project dependencies
- Run `make spin_everthing up`

## Data Tooling

### Airflow

The airflow configuration is stored within `airflow_pipelines`.

Currently, this includes local tooling for airflow which can be accessed with the following commands:

- `start_local_airflow`: Starts a local instance of an airflow cluster
- `stop_local_airflow`: Stops the airflow cluster
- `connect_to_airflow`: Enables you to connect to airflow through the UI (username: admin, password: admin)

This will create a kind cluster (consisting of a number of docker containers) which can be accessed with the command `kind get clusters`

You can view the actual pods by running: `kubectl get pods  --namespace airflow` and the logs from a pod by
`kubectl logs airflow-webserver-774654c96b-x2zv8 --namespace airflow  -c webserver`. You will need to change the webserver name to match the name of your
webserver. You can also optionally add a `-f` flag to follow the logs.

Current dags include:

- prefix_sensor_deferred: A Dag to demo using deferred operators for prefixes
- trigger_cloudrun: A deferred operator which triggers a cloudrun and waits for the return result
- census_public_census_v1: A demo dag which is autogenerated from metadata. This dag includes DBT and PySpark

### Terraform_CDK

- This is a demo of using the CDK to deploy tables from metadata. It uses the metadata defined in the metadata folder to create buckets and datasets.

### PySpark

There is pyspark code configured in the pyspark directory. This is currently only for the purpose of loading CSV files into BigQuery

This can be triggered from within airflow (see the census_public_census_v1 dag)
Code can be deployed using the pyspark build command.

### DBT

DBT models. Models can be generated directly from metadata using the `make dbt_create_models`. Currently, this just creates a view with redacted pii data.
