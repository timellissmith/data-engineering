# `cookiecutter-data-engineering` README

This repo contains a template which can be used to build out new Python
packages within CTS.

## Usage

1. Install `cookiecutter` on your system (or use the provided devcontainer):

    ```sh
    pip3 install cookiecutter
    ```

1. download the template you want to use and customise it:

    ```sh
    cookiecutter git@github.com:Cloud-Technology-Solutions/cookiecutter-data-engineering.git
    ```

## Pre-requisites

### Software packages
Please ensure that the following software is installed on your local machine:

- [Kind](https://kind.sigs.k8s.io/)
- [Helm](https://helm.sh/)
- [jq](https://stedolan.github.io/jq/)
- [yq](https://github.com/mikefarah/yq)


## Get started quickly

- Run poetry shell to create a virtual environment
- Run poetry install to install the project dependencies
- Run `make spin_everthing up`

## Data Tooling

### Airflow

The airflow configuration is stored within `airflow_pipelines`.

Currently, this includes local tooling for airflow which can be accessed with the following commands:

- `start_local_airflow`: Starts a local instance of an airflow cluster
- `stop_local_airflow`: Stops the airflow cluster
- `connect_to_airflow`: Enables you to connect to airflow through the UI (username: admin, password: admin)

This will create a kind cluster (consisting of a number of docker containers) which can be accessed with the command `kind get clusters`

You can view the actual pods by running: `kubectl get pods  --namespace airflow` and the logs from a pod by
`kubectl logs airflow-webserver-774654c96b-x2zv8 --namespace airflow  -c webserver`. You will need to change the webserver name to match the name of your
webserver. You can also optionally add a `-f` flag to follow the logs.

Current dags include:

- prefix_sensor_deferred: A Dag to demo using deferred operators for prefixes
- trigger_cloudrun: A deferred operator which triggers a cloudrun and waits for the return result
- census_public_census_v1: A demo dag which is autogenerated from metadata. This dag includes DBT and PySpark

### Terraform_CDK

- This is a demo of using the CDK to deploy tables from metadata. It uses the metadata defined in the metadata folder to create buckets and datasets.

### DBT

DBT models. Models can be generated directly from metadata using the `make dbt_create_models`. Currently, this just creates a view with redacted pii data.
